\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
 
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{float}
 
\geometry{
 a4paper,
 left=15mm,
 top=15mm,
 bottom=25mm
 }
\parindent0pt
\graphicspath{ {images/} } 
\newcommand{\R}{\mathbb{R}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\argmin}{\arg\!\min} 
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}
 
 % -------------------------------------------------------------
\title{Numerical Methods for CSE}
\author{Nino Scherrer \thanks{based on NumCSE script from Prof. Dr. Hiptmaier}}
\date{FS2016}
 
\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}
 
 
 % -------------------------------------------------------------
\section{General}

\subsection{Floating Point numbers}

\subsection{Dense Matrix}


\subsection{Sparse Matrix}
“almost all” entries = 0 /“only a few percent of” entries $\not =$ 0"

\subsubsection{Build Matrix using Triples}

\begin{lstlisting}[language=C++, caption=Triplet example]
//Definition of a SparseMatrix
SparseMatrix<double> C(n,m);

//TripletList to save some triplets
std::vector<Triplet<double>> tripletList;

//Triplet entry
Triplet<double> triplet (i, j, value); 	 
//Saving Triplet in TripletList
tripletList.push_back(triplet);


//Fill Sparse Matrix with triplets in tripletList
C.setFromTriplets(tripletList.begin(), tripletList.end());
//Compress Sparse Matrix
C.makeCompressed();
\end{lstlisting}

\subsubsection{Special storage formats}
\paragraph{- COO (Triplet/Coordinate format)}
\paragraph{- CRS (Compressed row storage)}
\hspace{2mm}

\includegraphics[width=0.8\textwidth]{SparseMatrix_CRS.png}




% -------------------------------------------------------------
% Machine Arithmetic
% -------------------------------------------------------------
\newpage
\section{Machine Arithmetic}

$\rightarrow$ Computers cannot compute “properly” in $\R$: \\
(numerical computations may not respect the laws of analysis and linear algebra)\newline

$\M \subset \R$ is the set of \textbf{machine numbers} (finite, discrete) \\
$\rightarrow$ not closed under elementary arithmetic operations! \\

\begin{tabular}{lll}
	$op$: 	& 		$\R \times \R \rightarrow \R$ & 			$op \in \lbrace +, -, *, / \rbrace$	\\
			& 		$\M \times \M \not\rightarrow \M$  											\\
																								\\
	$\widetilde{op}$: &  		$\M \times \M \rightarrow \M$ 	& $\Rightarrow$ $\widetilde{op} = rd \circ op \quad $ (on computers) 
\end{tabular}

\begin{center}
	\includegraphics[width=1.0\textwidth]{rounding_function.png}
\end{center}

\begin{tcolorbox}
\hspace{3mm}	
	For $\star \in \lbrace +, -, *, / \rbrace$: \quad $x \widetilde{\star} y = rd(x \star y)$  \quad \textit{where rd(...) is the rounding function} 
\hspace{3mm}	
\end{tcolorbox}
\hspace{3mm}	

% ----------------------
% Controlling roundoff error	
\subsection{Controlling roundoff error}	
\textbf{EPS} (largest relative error) := $\max\limits_{x \in I\setminus \lbrace 0 \rbrace} \frac{|rd(x) - x|}{|x|}$ \quad \textit{where $I = \lbrack \min |\M|, \max |\M| \rbrack$}
\\

... can also be computed by the defining parameters B (base) and m (length of mantissa): \\
\begin{equation*}
		EPS = \frac{1}{2} B^{1-m}
\end{equation*}
		
\begin{center}
	\includegraphics[width=1.0\textwidth]{axiom_roundoff_analysis.png}
\end{center}
	
\subsubsection{Getting machine precision with c++}
\begin{lstlisting}[language=C++]
	#include <iostream>
	#include <limits> 	// get various properties of arithmetic types
	int main() {
		std::cout.precision(15); 	// set floating point precision
		std::cout << std::numeric_limits<double>::epsilon() << std::endl;
	}
\end{lstlisting}

\textbf{Output:} \quad $2.22044604925031e^{-16}$ \\
\pagebreak

\subsubsection{Experiment - Adding EPS to 1}
\begin{lstlisting}[language=C++]
	cout.precision(25);
	double eps = numeric_limits<double>::epsilon();
	cout << fixed << 1.0 + 0.5*eps << endl
			<< 1.0 - 0.5*eps	<< endl
			<< (1.0 + 2/eps) - 2/eps << endl;			// cancellation here
\end{lstlisting}

\textbf{Output:} 

\begin{lstlisting}[language=c++]
1.000000000000000000000000
0.9999999999999998889776975
0.0000000000000000000000000
\end{lstlisting}


\hspace{5mm}	
\subsubsection{Testing equality with zero $|$ x == 0.0} 
\textbf{Numerical crime} for result of floating point computations \\
$\rightarrow$ Replace with test of relative smallness! \\
	
	... example of testing with relative smallness ...
	
\hspace{6mm}

	
% -----------
% Cancellation 
\subsection{Cancellation}
 

 \begin{minipage}{0.6\textwidth}
 	... = \textbf{subtraction of almost equal numbers} \\
 $\blacktriangleright$ extreme ampilifcation of relative errors \\
 $\blacktriangleright$ if inevitable, substraction prone to cancellation should be done as early as possible
 \end{minipage}
\hfill
 \begin{minipage}{0.3\textwidth}\raggedleft
 		 \includegraphics[width=0.8\textwidth]{cancellation_illustration.png}
 \end{minipage}


\hspace{5mm}
\subsubsection{Example  1 - Cancellation when evaluating difference quotients}
\begin{equation*}
	f'(x) \approx \frac{f(x+h) - f(x)}{h} \quad \quad , |h|<<1	
\end{equation*}
\hspace{10mm}
$\Rightarrow$ Cancellation will happen here: $f(x+h) - f(x)$


\subsubsection{Example  2 - Cancellation in Gram-Schmidt orthogonalisation}

\begin{minipage}{0.6\textwidth}
Cancellation when computing orthogonal projection of vector a onto space spanned by vector b.
\begin{equation*}
	p = a - \frac{a \cdot b }{b \cdot b}b
\end{equation*}
If a, b are nearly parallel: $p \approx 0$ $\rightarrow$ Cancellation
\end{minipage}
\begin{minipage}{0.3\textwidth}\raggedleft
 		 \includegraphics[width=1.0\textwidth]{cancellation_gramSchmidt.png}
 \end{minipage}



\subsubsection{Tricks to avoid cancellation}
 
 \begin{tcolorbox}
 \textbf{USE OF ALTERNATIVE FORMULAS:}
 \begin{itemize}
  \item Use mathmatic identities (trigonometric, definitions, ...)
  \item Approximation with Taylor Series $\rightarrow$ Choosing m large enough
  \item Sometimes: Use of case distinctions
\end{itemize}
\end{tcolorbox}


% ----------------
% Numerical stability
\subsection{Numerical stability}

\subsubsection{Problem description}
\begin{minipage}{0.6\textwidth}
	\begin{equation*}
	F: X \subset \R^{n} \longrightarrow Y \subset \R^{m}	
	\end{equation*}
	\begin{center}
		(X := data space and Y := result space)
	\end{center}
\end{minipage}
\begin{minipage}{0.3\textwidth}\raggedleft
	\begin{center}
	 \includegraphics[width=1.0\textwidth]{stable_problem_desc.png}
\end{center}
\end{minipage}


\subsubsection{Numerical algorithm}
\begin{itemize}
	\item Problem: \quad  \quad $F: X \subset \R^{n} \longrightarrow Y \subset \R^{m}	$
	\item Algorithm: \quad  $\tilde{F}: X \rightarrow \tilde{Y} \subset \M$
\end{itemize}



\subsubsection{Stable Algorithm (= Good Algorithm)}
\begin{center}
	 \includegraphics[width=1.0\textwidth]{stable_algorithm1.png}
\end{center}

\begin{center}
	 \includegraphics[width=0.6\textwidth]{stable_algorithm2.png}
\end{center}

% -------------------------------------------------------------
% Direct Methods for Linear Systems of Equations
% -------------------------------------------------------------
\newpage
\section{Direct Methods for Linear Systems of Equations}




% -------------------------------------------------------------
% Direct Methods for Linear Least Squares Problems
% -------------------------------------------------------------
\newpage
\section{Direct Methods for Linear Least Squares Problems}

- uebersichtstabelle

\subsection{General - Least Squares}

\subsection{Solution Concepts}

\subsection{Normal Equation method (std. / extd.)}

\subsection{Orthogonal Transformation method}
 
\subsection{Singular Value Decomposition (SVD)}


%--------------------------------
\subsection{Total Least Squares}
We have generally considered overdetermined linear systems of equations $Ax = b$, for which only the right handside vector b was affected by measurement errors. But it is also possible that A and b are perturbed (affected by measurements errors)!

\begin{tcolorbox}
\textbf{Given:}\\
$Ax = b$ (overdetermined linear system of equations) \\
$m \geq n$, $rank(A)=n$ ,  $b \in \R^{m}$\vspace{2mm}\\
\textbf{Known:}  \\
LSE would be solvable if A, b were not perturbed  \vspace{2mm}\\
\textbf{Task:} \\
Find the nearest solvable LSE \\
$\rightarrow  \hat A \in \R^{m,n}$ ,  $\hat b \in \R^{m}$ with: \\
\[ \lVert \lbrack A \text{ }b \rbrack - \lbrack \hat A \text{ } \hat b \rbrack\rVert_{F} \rightarrow min, \quad \hat b \in \mathcal{R}(\hat A) \] 
\end{tcolorbox}


$\Rightarrow \lbrack \hat A \text{ } \hat b \rbrack \in \R^{m, n+1}$ \textbf{is the rank-n best approximation of}  $\lbrack A \text{ }b \rbrack$ \\

\textbf{Calculation:} \\
(1) SVD of $\lbrack A \text{ }b \rbrack = U \Sigma V^{T}$, \quad \quad $V \in \R^{n+1, n+1}$ \\
\[ \lbrack A \text{ }b \rbrack =  U \Sigma V^{T} = \sum_{j=1}^{n+1} \sigma_j (U)_{:,j} (V)_{:,j}^{T} \quad \Rightarrow \quad  \lbrack \hat A \text{ } \hat b \rbrack = \color{red}\sum_{j=1}^{n} \color{black}\sigma_j (U)_{:,j} (V)_{:,j}^{T} \]
\begin{center}
$\Rightarrow \hat A = (\lbrack \hat A \text{ } \hat b \rbrack )_{1:n, 1:n}$, \quad $\hat b = (\lbrack \hat A \text{ } \hat b \rbrack )_{1:n, n+1}$ \vspace{2mm}\\
	Due to orthogonality: $\lbrack \hat A \text{ } \hat b \rbrack (V)_{:n+1} = 0$
	\vspace{10mm}\\

	$x = \hat A^{-1} \hat b =\color{red} ... (correct formula)...$
 
\end{center}


%--------------------------------
\subsection{Constrained Least Squares}

!! Some data are considered accurate $\rightarrow$ mix interpolation and least squares fitting

\begin{tcolorbox}
\textbf{Linear least squares problem with linear constraint:}
\vspace{2mm} \\
Given: \\
$A \in \R^{m,n}, m \geq n, rank(A) = n, b \in \R^{m}$\\
 $C \in \R^{p,n}, m < n, rank(C) = p, d \in \R^{p}$ \\

Find: \\
$x \in \R^{n}$ with $ \lVert Ax - b\rVert_{2} \rightarrow min $, \quad \quad $Cx =d$ (linear constraint)	
\end{tcolorbox}

\subsubsection{Solution with normal equations (and Lagrange parameter)}

\textbf{Idea:} coupling the constraint using the Lagrange multiplier $m \in R^{p}$

\begin{equation*}
	x = \argmin_{x \in \R^{n}}  \max_{m \in \R^{p}} L(x,m)  \quad \longrightarrow \quad L(x,m) ) := \frac{1}{2} \lVert Ax - b\rVert^{2} + m^{T} (Cx - d)
\end{equation*}

Simple heuristics behind Lagrange multipliers: $\max_{m \in \R^{p}} L(x,m) = \infty $ in case of: $Cx \not = d$

\begin{equation*}
	\text{Augmented normal equation: \quad}
  	\begin{bmatrix}
  		A^{T}A 	& C^{T} \\
  		C 			& 0 
  	\end{bmatrix}
  	\begin{bmatrix}
  		x \\
  		m
  	\end{bmatrix} 
  	=
  	\begin{bmatrix}
  		A^{T}b \\
  		d
  	\end{bmatrix} 
\end{equation*}

\subsubsection{Solution with SVD}
\textbf{Idea:} identify the subspace in which the solution can vary without violating the constraint. Since C has full
rank, this subspace agrees with the nullspace/kernel of C.
\vspace{3mm} \\
(1) Compute orthonorm. basis of $\mathcal{N}(C)$ using SVD \\
\[ ... \text{fill with explanation} ... \]
(2) Calculate particular solution of the constraint equation: \\
\[ x_0 := V_1\Sigma^{-1}U^{T}d \] 
(3) Representation of the solution x: \\
\[ x =x_0 + V_2y, \quad  y \in \R^{n-p}\]
(4) Insert in Std. LSE representation:  \\
\[ \lVert A(x_0 + V_2y) - b\rVert_{2} \rightarrow min \]

%-------------------------------------------------------
% Operation Costs
%-------------------------------------------------------
\newpage
\section{Operation Costs}

Consider $x, y \in \mathbb{R}^n$, $A \in \mathbb{R}^{m,n}$, $B \in \mathbb{R}^{n,k}$, $C \in \mathbb{R}^{g,h}$ and $D \in \mathbb{R}^{n,n}$

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Operation} & \textbf{Description} & \textbf{Complexity} \\ \hline
Dot Product 	   & $x^Ty$ 				  & $O(n)$              \\ \hline
Tensor Product	   & $xy^T$               & $O(mn)$			    \\ \hline
Matrix Product	   & $AB$             	  & $O(mnk)$			    \\ \hline
Kronecker Product  & $A \otimes C$        & $O(mngh)$			\\ \hline
Gaussian Elimination (without pivoting) & $Ax = y$        & $O(n^3)$			\\ \hline
LU decomposition (setup phase) & $Dlu = D$.lu()  & $O(n^3)$			\\ \hline
LU decomposition (elimination phase) & $Dlu$.solve($x$)  & $O(n^2)$			\\ \hline
Householder QR & $A$.householderQr()  & $O(mn^2)$			\\ \hline
Economical SVD & & $O(\min(m, n)^2\cdot\max(m,n))$ \\ \hline
\end{tabular}
\end{table}

\end{document}