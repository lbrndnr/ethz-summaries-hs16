\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{float}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage[labelformat=empty]{caption}
\usepackage{booktabs}

\geometry{
	a4paper,
 	left=20mm,
 	top=20mm,
 	bottom=20mm,
}

\setlength{\parindent}{0pt}

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}

\begin{document}

\section{Wahrscheinlichkeiten}

\begin{table}[H]
\centering
\begin{tabular}{|p{2cm}|p{6cm}|p{6cm}|}
\hline
                   & \textbf{with repetition} & \textbf{without repetition} \\\hline
\textbf{ordered}   & $n^k$                         							 	  & $\frac{n!}{(n-k)!}$    \\\cline{2-3}
				   & A passcode of length $n$ with $k$ different digits           & How many ways can $k$ places be awarded to $n$ people                       \\\hline
\textbf{unordered} & $\bigl(\begin{smallmatrix}n+k-1\\k \end{smallmatrix} \bigr)$ & $\bigl(\begin{smallmatrix}n\\k \end{smallmatrix} \bigr) = \frac{n!}{k!(n-k)!}$ \\\cline{2-3}
				   & Distribute $k$ bananas (identical) to $n$ monkeys (identical)   & Select $k$ from $n$ objects                             \\\hline
\end{tabular}
\end{table}

\begin{description}[labelindent=16pt,style=multiline,leftmargin=9cm, noitemsep]
	\item[Ereignisraum $\Omega$:] Die Menge aller m{\"o}glichen Ergebnisse
	\item[Elementarereignis $\omega \in \Omega$:] Elemente vom Ereignisraum
	\item[Potenzmenge $\mathcal{P}(\Omega)$ oder $2^\Omega$:] Menge aller Teilmengen von $\Omega$
	\item[prinzipielles Ereigniss $A \subseteq \Omega$:] Kollektion von Elementarereignissen
	\item[Klasse der beobachtbaren Ereignisse $\mathcal{F}$:] Teilmenge der Potzenmenge $\mathcal{P}(\Omega)$ (alle m{\"o}glichen Ereignisse)
	\item[Wahrscheinlichkeitsmass $P: \mathcal{F} \mapsto \lbrack 0,1 \rbrack $:] Weist einem Ereignis eine Wahrscheinlichkeit zu
\end{description}

Es gelten:
\begin{equation*}
\begin{split}
	P[A^c] & = 1 - P[A] \\
	P[A \cup B] & = P[A] + P[B] - P[A \cap B] \quad\text{(Additionsregel)} \\
	P[A \cap B] & = P[B|A] \cdot P[A] \quad\text{(Multiplikationsregel)} \\
	P[A \backslash B] & = P[A] - P[A \cap B] \\
	P[A \cap B \cap C] + P[A \cap B \cap C^c] & = P[A \cap B]
\end{split}
\end{equation*}

\subsection{Diskrete Wahrscheinlichkeitsr{\"a}ume}

Ist $\Omega = {\omega_1, \omega_2,...,\omega_N}$ endlich mit $|\Omega| = N$ und $\mathcal{F} = 2^\Omega$ und sind die Elementarereignisse alle gleich wahrscheinlich, so ist $\Omega$ ein \textbf{Laplace-Raum} und $P$ die \textbf{diskrete Gleichverteilung} auf $\Omega$.

\begin{equation*}
	P[A] = \frac{\text{Anzahl der Elementarereignisse in A}}{\text{Anzahl der Elementarereignisse in $\Omega$}} = \frac{|A|}{|\Omega|}
\end{equation*}

\subsection{Bedingte Wahrscheinlichkeiten}

Die bedingte Wahrscheinlichkeit von $B$ unter der Bedingung, dass $A$ eintritt ($A$ gegeben) ist definiert durch:
\begin{equation*}
\begin{split}
	P[B|A] & := \frac{P[B \cap A]}{P[A]} \\
	P[B^c|A] & = 1-P[B|A] \\
\end{split}
\end{equation*}

\emph{Veranschaulichung:} $A$ ist eingetreten. Da $A$ und $B$ von einander abh{\"a}ngen, l{\"a}sst sich somit auch die Wahrscheinlichkeit berechnen, dass $B$ eingetreten ist.

\paragraph{Satz der totalen Wahrscheinlichkeit:} Sei $A_1,...,A_n$ eine Zerlegung von $\Omega$ in paarweise disjunkte Ereignisse, dann gilt f{\"u}r ein beliebiges Ereignis $B$:
\begin{equation*}
	P[B] = \sum_{i=1}^n P[B|A_i] P[A_i]
\end{equation*}
\begin{center}
	\includegraphics[width=200pt]{images/totale_wahrscheinlichkeit}
\end{center}

\paragraph{Formel von Bayes:} Ist $A_1,...,A_n$ eine Zerlegung von $\Omega$ mit $P[A_i] > 0$ und $B$ ein Ereignis mit $P[B] > 0$ so gilt f{\"u}r jedes $k$:
\begin{equation*}
	P[A_k|B] = \frac{P[B|A_k]P[A_k]}{\sum_{i=1}^n P[B|A_i]P[A_i]}
\end{equation*}

\subsection{Unabh{\"a}ngigkeit}

Zwei Ereignisse $A, B$ sind unabh{\"a}ngig (stochastisch), falls gilt:
\begin{equation*}
\begin{split}
	P[A \cap B] & = P[A]P[B] \quad\text{(Produktregel)}\\
	P[B|A] & = P[B] \\
	P[A|B] & = P[A] \\
\end{split}
\end{equation*}

\emph{Veranschaulichung:} Die Tatsache, dass eines der Ereignisse eingetreten ist, hat keinen Einfluss auf die Wahrscheinlichkeit des anderen. Unabh{\"a}ngigkeit und disjunkte Mengen sind jedoch \emph{nicht} das selbe.

\section{Diskrete Zufallsvariablen und Verteilungen}

\begin{description}[labelindent=16pt,style=multiline,leftmargin=8cm, noitemsep]
	\item[Zufallsvariable $X: \Omega \mapsto \mathbb{R}$:] Eine eindeutige Zuordnung, die jedem Elementarereignisses im Ereignisraum eine Zahl zuordnet.
	\item[Verteilungsfunktion $F_X: \mathbb{R} \mapsto \lbrack 0,1 \rbrack$:] $F_X(t) := P[X \leq t] := P[\{\omega | X(\omega) \leq t\}]$ \\ Der Wert an der Stelle $x$  entspricht der Wahrscheinlichkeit, dass die zugeh{\"o}rige Zufallsvarible $X$ einen Wert kleiner oder gleich $x$ annimmt.
	\item[Gewichtsfunktion $p_X:\mathcal{W}(X) \mapsto \lbrack 0,1 \rbrack$:] $p_X(x_k) := P[X = x_k] = P[\{\omega | X(\omega) = x_k\}]$ \\ Die Summe aller Gewicht aller Ereignisse muss zusammen 1 ergeben.
	\item[Indikatorfunktion $I_A$:] $I_A(\omega) = 1$, falls $\omega \in A$, $0$ sonst. Eine diskrete Zufallsvariable, falls $\Omega$ endlich und abz{\"a}hlbar ist.
\end{description}

\subsection{Erwartungswert}

Sei $X$ eine diskrete Zufallsvariable mit Gewichtsfuntkion $p_X(x)$, dann ist der Erwartungswert $\E[X]$ wie folgt definiert:
\begin{equation*}
	\E[X] := \sum_{x_k \in \mathcal{W}(X)} x_kp_X(x_k)
\end{equation*}

Ist diese Reihe nicht \emph{absolut konvergent}, so existiert der Erwartungswert nicht.

Sei weiter $Y = g(X)$ eine Zufallsvariable gem{\"a}ss $g: \mathbb{R}\mapsto\mathbb{R}$ abh{\"a}ngig von $X$, so gilt:
\begin{equation*}
	\E[Y] = \E[g(X)] = \sum_{x_k \in \mathcal{W}(X)} g(x_k)p_X(x_k)
\end{equation*}

F{\"u}r zwei Zufallsvariablen $X$ und $Y$, die jeweils einen Erwartungswert definiert haben, gilt:
\begin{description}[labelindent=16pt,style=multiline,leftmargin=4cm, noitemsep]
	\item[Monotonie:] Ist $X \leq Y$ (bzw. $\forall\omega: X(\omega) \leq Y(\omega)$), so gilt auch $\E[X] \leq \E[Y]$
	\item[Linearit{\"a}t:] F{\"u}r beliebige $a,b \in \mathbb{R}$ gilt $\E[aX + b] = a\E[X]+b$
	\item[Werte in $\mathbb{N}_0$:] Falls $X$ nur Werte in $\mathbb{N}_0$ annimmt, so gilt \\ $\E[X] = \sum^\infty_{j=1}P[X \geq j] = \sum^\infty_{l=0}P[X > l]$
\end{description}

\subsection{Varianz}

Sei $X$ eine diskrete Zufallsvariable und ist $\E[X] < \infty$, so ist die \textbf{Varianz} definiert als:
\begin{equation*}
	\Var[X] := \E[(X-\E[X])^2] = \sum_{x_k \in \mathcal{W}(X)}(x_k - \E[X])^2 p_X(x_k)
\end{equation*}

Dementsprechend ist die \textbf{Standardabweichung} von $X$ gleich
\begin{equation*}
	\text{sd}(X) = \sigma(X) := \sqrt{\Var[X]}
\end{equation*}

Die Varianz wie auch die Standardabweichung beschreiben die \textbf{Streuung} der Verteilung von $X$. F{\"u}r beliebiges $X$ und $Y = aX +b$ gilt:

\begin{equation*}
\begin{split}
	\Var[X] & = \E[X^2] - \E[X]^2 \\
	\Var[Y] & = \Var[aX +b] = a^2\Var[X]
\end{split}	
\end{equation*}

\subsection{Gemeinsame Verteilungen}

\begin{equation*}
\begin{split}
	\text{Gemeinsame Gewichtsfunktion } p: \mathbb{R}^n \mapsto [0,1] \qquad & p(x_1,...,x_n) := P[X_1 = x_1,..., X_n = x_n] \\
	\text{Gemeinsame Verteilungsfunktion } F: \mathbb{R}^n \mapsto [0,1] \qquad & F(x_1,...,x_n) = P[X_1 \leq x_1,..., X_n \leq x_n] \\
	& = \sum_{y_1 \leq x_1,...,y_n \leq x_n} p(y_1,...,y_n)
\end{split}
\end{equation*}

\emph{Veranschaulichung:} F{\"u}r die Zufallsvariablen $X$ und $Y$ beschreibt die gemeinsame Verteilungsfunktion die Wahrscheinlichkeit, dass der Vektor $(X,Y)$ in ein unendliches Rechteck der Form $(-\infty, X] \times (-\infty,y]$ f{\"a}llt.

\subsubsection{Randverteilungen}

Haben $X$ und $Y$ die gemeinsame Verteilungsfunktion $F$, so ist die Verteilungsfunktion der Randverteilung von $X$ gegeben durch:
\begin{equation*}
	F_X(x) := P[X \leq x] = P[X \leq x, Y < \infty] = \lim_{y \rightarrow \infty} F(x,y)
\end{equation*}

Analog dazu ist die Gewichtsfunktion der Randverteilung von $X$ gleich:
\begin{equation*}
	p_X(x) = P[X = x] = \sum_{y_i \in \mathcal{W}(Y)} P[X=x, Y=y_i] = \sum_{y_i \in \mathcal{W}(Y)} p(x, y_i) \text{ f{\"u}r } x \in \mathcal{W}(X)
\end{equation*}

\subsubsection{Unabh{\"a}ngigkeit}

Die Zufallsvariablen $X_1,...,X_n$ heissen \textbf{unabh{\"a}ngig}, falls
\begin{equation*}
\begin{split}
		\text{entweder} \quad & F(x_1,...,x_n) = F_{X_1}(x_1) \cdot\cdot\cdot F_{X_n}(x_n)  \\
		\text{oder} \quad & p(x_1,...,x_n) = p_{X_1}(x_1) \cdot\cdot\cdot p_{X_n}(x_n)
\end{split}
\end{equation*}

\begin{equation*}
	\text{unabh{\"a}ngig} \Rightarrow \text{paarweise unabh{\"a}ngig} \Rightarrow \text{unkorreliert}
\end{equation*}

\subsection{Funktionen von mehreren Zufallsvariablen}

Seien $X_1,...,X_n$ diskrete Zufallsvariablen mit endlichen Erwartungswerten $\E[X_1],...,\E[X_n]$...

\subsubsection{Summen von Erwartungswerte}

Sei weiter $Y = a + \sum_{l=1}^n b_lX_l$ mit Konstanten $a,_1,...,b_n$, dann gilt:
\begin{equation*}
	\E[Y] = a + \sum_{l=1}^n b_l\E[X_l]
\end{equation*}

Es folgt, dass Erwartungswerte sich \textbf{immer linear verhalten}. Der Erwartungswert einer Summe von Zufallsvariablen ist also immer die Summe der Erwartungswerte.
	
\subsubsection{Summen von Varianzen}

Die \textbf{Kovarianz} von $X$ und $Y$ ist wie folgt definiert:
\begin{equation*}
	\Cov(X,Y) = \E[XY] - \E[X]\E[Y] = \E[(X - \E[X])(Y-\E[Y])]
\end{equation*}

Es gelten:
\begin{equation*}
\begin{split}
	& \Cov(X, X) = \Var(X) \\
	& \Cov(X, Y) = 0 \Rightarrow \text{$X$ und $Y$ sind unkorreliert}
\end{split}
\end{equation*}

Daraus folgt die \textbf{allgemeine Summenformel f{\"u}r Varianzen}:

\begin{equation*}
\begin{split}
	\text{korreliert:} \quad & \Var\Bigg[\sum_{i=1}^n X_i \Bigg] = \sum_{i=1}^n\Var[X_i] + 2\sum_{i<j}\Cov(X_i, X_j) \\
	\text{unkorreliert:} \quad & \Var\Bigg[\sum_{i=1}^n X_i \Bigg] = \sum_{i=1}^n\Var[X_i]
\end{split}
\end{equation*}

\subsubsection{Produkte von Zufallsvariablen und Varianzen}

Falls $X_1,...X_n$ unabh{\"a}ngig sind und der Erwartungswert links {\"u}berhaupt existiert, gilt

\begin{equation*}
	E\Bigg[\prod_{i=1}^n X_i\Bigg] = \prod_{i=1}^n \E[X_i]
\end{equation*}

In diesem Fall sind dann $X_1,...,X_n$ paarweise unkorreliert.

\subsection{Bedingte Verteilungen}

Seien $X$ und $Y$ diskrete Zufallsvariablen mit gemeinsamer Gewichtsfunktion $p(x, y)$. Die bedingte Gewichtsfunktion von $X$, gegeben dass $Y = y$, ist definiert durch
\begin{equation*}
	p_{X|Y}(x|y) := P[X=x|Y=y] = \frac{P[X=x,Y=y]}{P[Y=y]} = \frac{p(x,y)}{p_Y(y)}
\end{equation*}

\section{Wichtige Diskrete Verteilungen}

\subsection{Diskrete Gleichverteilung}

Alle Elementarereignisse treten mit der gleichen Wahrscheinlichkeit auf.

\begin{equation*}
	p_X(x_k) = P[X = x_k] = \frac{1}{N}
\end{equation*}

\subsection{Bernoulli-Verteilung}

Bei einem \textbf{0-1-Experiment} ist die Gewichtsfunktion mit einer \textbf{Bernoulli-Verteilung} wie folgt definiert:
\begin{equation*}
	p_X(x) = p^x(1-p)^{1-x} \quad\text{für } k = 1, 2, ..., N
\end{equation*}

Es folgt, dass folgende Formeln gelten:
\begin{equation*}
\begin{split}
	p_X(1) & = P[X = 1] = p \\
	p_X(0) & = P[X = 0] = 1-p \\
\end{split}
\end{equation*}

Anders ausgedrückt ist $X$ die Anzahl Erfolge bei einem einzelnen 0-1-Experiment mit \textbf{Erfolgsparameter} $p$, liefert also nur die Werte 1 oder 0. Wir schreiben $X \sim Be(p)$.

\textbf{Erwartungswert} und \textbf{Varianz} verhalten sich bei einer Bernoulli-verteilten Zufallsvariable wie folgt:
\begin{equation*}
\begin{split}
	\E[X] & = 1 \cdot P[X = 1] + 0 \cdot P[X = 0] = p \\
	\Var[X] & = \E[X^2] - (\E[X])^2 = p(1-p) \quad \text{da } X^2 = X
\end{split}
\end{equation*}

\subsection{Binomialverteilung}

Die \textbf{Binomialverteilung} mit Parametern $n$ und $p$ beschreibt $n$ \textbf{0-1-Experimente} mit \textbf{Erfolgsparameter} $p$. \\
Der Wertebereich ist also $\mathcal{W} = \{0, 1, ..., n\}$. Die Gewichtsfunktion ist gleich
\begin{equation*}
	p_X(k) = P[X = k] = \binom{n}{k}p^k(1-p)^{n-k} \quad\text{für } k = 0, 1, ..., n
\end{equation*}

Wir schreiben $X \sim Bin(n,p)$. Falls $n = 1$ gilt $Bin(n,p) = Be(p)$

\subsection{Geometrische Verteilung}

Betrachten wir eine unendliche Folge von unabhängigen \textbf{0-1-Experimenten} mit \textbf{Erfolgsparameter} $p$, dann ist $X$ der Index der ersten Eins in der zufälligen 0-1-Folge. \\
Der Wertebereich ist dementsprechend $\mathcal{W} = \mathbb{N}$ und die Gewichtsfunktion gleich
\begin{equation*}
	p_X(k) = P[X = k] = p(1-p)^{k-1} \quad\text{für } k = 1, 2, 3,...
\end{equation*}

Wir schreiben $X \sim Geom(p)$. $X = k$ heisst gerade, dass $A_1,..., A_{k-1}$ nicht eintreten und $A_k$ als erstes eintritt. Weiter ist der \textbf{Erwartungswert} und die \textbf{Varianz} gleich
\begin{equation*}
\begin{split}
	\E[X] & = \sum_{l=0}^\infty P[X > l] = \frac{1}{p} \\
	\Var[X] & = \frac{1-p}{p^2}
\end{split}
\end{equation*}

\subsection{Negativbinomiale Verteilung}

Eine \textbf{negativbinomiale Verteilung} mit Parametern $r$ und $p$ beschreibt eine unendliche Folge von unabhängigen \textbf{0-1-Experimenten} mit \textbf{Erfolgsparameter} $p$ und dem $r$-ten Erfolg. \\
Der Wertebereich ist $\mathcal{W} = \{r, r+1, r+2,...\}$ und die Gewichtsfunktion
\begin{equation*}
	p_X(k) = P[X = k] = \binom{k-1}{r-1}p^r(1-p)^{k-r} \quad\text{für } k = r, r+1, r+2, ...
\end{equation*}

Wir schreiben kurz $X \sim NB(r,p)$. Es gilt:
\begin{equation*}
\begin{split}
	\E[X] & = \sum_{i=1}^r E[X_i] = \frac{r}{p} \\
	\Var[X] & = \sum_{i=1}^r \Var[X_i] = \frac{r(1-p)}{p^2}
\end{split}
\end{equation*}

\subsection{Hypergeometrische Verteilung}

In einer Urne seien $n$ Gegenstände, davon $r$ vom Typ 1 und $n -r$ vom Typ 2. Man zieht ohne Zurücklegen $m$ der Gegenstände; die Zufallsvariable $X$ beschreibe die Anzahl der Gegenstände vom Typ 1 in dieser Stichprobe vom Umfang $m$. Dann hat $X$ eine \textbf{hypergeometrische Verteilung} mit Parametern $n \in \mathbb{N}$ und $m,r \in \{1, 2, ..., n\}$. \\
Der Wertebereich ist gleich $\mathcal{W} = \{1, 2,... \min(r, m)\}$ und $X$ hat die Gewichtsfunktion
\begin{equation*}
	p_X(k) = \frac{\binom{r}{k}\binom{n-r}{m-k}}{\binom{n}{m}} \quad \text{für } k \in \mathcal{W}(X)
\end{equation*}

\subsection{Poisson-Verteilung}

Die \textbf{Poisson-Verteilung} mit Parameter $\lambda \in (0, \infty)$ ist eine Verteilung auf der Menge $\mathbb{N}_0$ mit Gewichtsfunktion
\begin{equation*}
	p_X(k) = e^{-\lambda}\frac{\lambda^k}{k!}\quad\text{für } k = 0, 1, 2,...
\end{equation*}

Wir schreiben kurz $X \sim \mathcal{P}(\lambda)$

\end{document}
